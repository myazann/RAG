[MPT-7B]
repo_id = mosaicml/mpt-7b-chat
context_length = 2048
min_GPU_RAM = <24

[FALCON-7B]
repo_id = tiiuae/falcon-7b-instruct
context_length = 2048
min_GPU_RAM = <24

[GPT4ALL-13B-GPTQ]
repo_id = TheBloke/GPT4All-13B-Snoozy-SuperHOT-8K-GPTQ
basename = gpt4all-snoozy-13b-superhot-8k-GPTQ-4bit-128g.no-act.order
context_length = 2048
min_GPU_RAM = <24

[VICUNA-7B-v1.5-GPTQ]
repo_id = TheBloke/vicuna-7B-v1.5-GPTQ
context_length = 4096
min_GPU_RAM = <24

[VICUNA-7B-v1.5-16K]
repo_id = lmsys/vicuna-7b-v1.5-16k
context_length = 16384
min_GPU_RAM = >24

[VICUNA-13B-v1.5-GPTQ]
repo_id = TheBloke/vicuna-13B-v1.5-GPTQ
context_length = 4096
min_GPU_RAM = <24

[LLAMA2-7B-HF]
repo_id = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = >24

[LLAMA2-7B-GPTQ]
repo_id = TheBloke/Llama-2-7b-Chat-GPTQ
basename = model
context_length = 4096
min_GPU_RAM = <24

[LLAMA2-13B-GPTQ]
repo_id = TheBloke/Llama-2-13B-chat-GPTQ
basename = model
context_length = 4096
min_GPU_RAM = <24

[STABLE-BELUGA-7B-GPTQ]
repo_id = TheBloke/StableBeluga-7B-GPTQ
basename = gptq_model-4bit-128g
context_length = 4096
min_GPU_RAM = <24

[STABLE-BELUGA-13B-GPTQ]
repo_id = TheBloke/StableBeluga-13B-GPTQ
basename = gptq_model-4bit-128g
context_length = 4096
min_GPU_RAM = <24

[CLAUDE-V1]
repo_id = claude-1.2
context_length = 100000
min_GPU_RAM = <24

[CLAUDE-V2]
repo_id = claude-2.0
context_length = 100000
min_GPU_RAM = <24

[LUNA-UNCENSORED]
repo_id = TheBloke/Luna-AI-Llama2-Uncensored-GPTQ
basename = model
context_length = 4096
min_GPU_RAM = <24