[VICUNA-7B-v1.5]
repo_id = lmsys/vicuna-7b-v1.5
context_length = 4096
min_GPU_RAM = 24

[VICUNA-7B-v1.5-AWQ]
repo_id = TheBloke/vicuna-7B-v1.5-AWQ
tokenizer = lmsys/vicuna-7b-v1.5
context_length = 4096
min_GPU_RAM = 8

[VICUNA-7B-16K-v1.5]
repo_id = lmsys/vicuna-7b-v1.5-16k
context_length = 16384
min_GPU_RAM = 24

[VICUNA-7B-16K-v1.5-AWQ]
repo_id = TheBloke/vicuna-7B-v1.5-16K-AWQ
tokenizer = lmsys/vicuna-7b-v1.5-16k
context_length = 16384
min_GPU_RAM = 8

[VICUNA-13B-v1.5]
repo_id = lmsys/vicuna-13b-v1.5
context_length = 4096
min_GPU_RAM = 34

[VICUNA-13B-v1.5-AWQ]
repo_id = TheBloke/vicuna-13B-v1.5-AWQ
tokenizer = lmsys/vicuna-13b-v1.5
context_length = 4096
min_GPU_RAM = 12

[VICUNA-13B-16K-v1.5]
repo_id = lmsys/vicuna-13b-v1.5-16k
context_length = 16384
min_GPU_RAM = 34

[VICUNA-13B-16K-v1.5-AWQ]
repo_id = TheBloke/vicuna-13B-v1.5-16K-AWQ
tokenizer = lmsys/vicuna-13b-v1.5-16k
context_length = 16384
min_GPU_RAM = 12

[LLAMA3-8B]
repo_id = meta-llama/Meta-Llama-3-8B-Instruct
context_length = 8192

[LLAMA3-8B-AWQ]
repo_id = casperhansen/llama-3-8b-instruct-awq
tokenizer = meta-llama/Meta-Llama-3-8B-Instruct
context_length = 8192

[LLAMA3-8B-PPLX]
repo_id = llama-3-8b-instruct
tokenizer = meta-llama/Meta-Llama-3-8B-Instruct
context_length = 8192

[LLAMA3-8B-GROQ]
repo_id = llama3-8b-8192
tokenizer = meta-llama/Meta-Llama-3-8B-Instruct
context_length = 8192

[LLAMA3-70B-PPLX]
repo_id = llama-3-70b-instruct
tokenizer = meta-llama/Meta-Llama-3-70B-Instruct
context_length = 8192

[LLAMA3-70B-GROQ]
repo_id = llama3-70b-8192
tokenizer = meta-llama/Meta-Llama-3-70B-Instruct
context_length = 8192

[LLAMA3-70B-GGUF]
repo_id = MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF
file_name = Meta-Llama-3-70B-Instruct.Q4_K_M.gguf
tokenizer = meta-llama/Meta-Llama-3-70B-Instruct
context_length = 8192

[LLAMA2-7B]
repo_id = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = 24

[LLAMA2-7B-AWQ]
repo_id = TheBloke/Llama-2-7B-Chat-AWQ
tokenizer = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = 8

[LLAMA2-13B]
repo_id = meta-llama/Llama-2-13b-chat-hf
context_length = 4096
min_GPU_RAM = 34

[LLAMA2-13B-AWQ]
repo_id = TheBloke/Llama-2-13B-chat-AWQ
tokenizer = meta-llama/Llama-2-13b-chat-hf
context_length = 4096
min_GPU_RAM = 12

[LLAMA2-70B]
repo_id = meta-llama/Llama-2-70b-chat-hf
context_length = 4096
min_GPU_RAM = 150

[LLAMA2-70B-AWQ]
repo_id = TheBloke/Llama-2-70B-Chat-AWQ
tokenizer = meta-llama/Llama-2-70b-chat-hf
context_length = 4096
min_GPU_RAM = 40

[GPT-3.5]
repo_id = gpt-3.5-turbo
context_length = 16384
min_GPU_RAM = 0

[GPT-4]
repo_id = gpt-4-turbo
context_length = 128000
min_GPU_RAM = 0

[GPT-4o]
repo_id = gpt-4o
context_length = 128000
min_GPU_RAM = 0

[GPT-4o-mini]
repo_id = gpt-4o-mini
context_length = 128000
min_GPU_RAM = 0

[CLAUDE-1.2]
repo_id = claude-instant-1.2
context_length = 100000
min_GPU_RAM = 0

[CLAUDE-2.0]
repo_id = claude-2.0
context_length = 100000
min_GPU_RAM = 0

[CLAUDE-2.1]
repo_id = claude-2.1
context_length = 200000
min_GPU_RAM = 0

[CLAUDE-3-OPUS]
repo_id = claude-3-opus-20240229
context_length = 200000
min_GPU_RAM = 0

[CLAUDE-3-SONNET]
repo_id = claude-3-sonnet-20240229
context_length = 200000
min_GPU_RAM = 0

[CLAUDE-3-HAIKU]
repo_id = claude-3-haiku-20240307
context_length = 200000
min_GPU_RAM = 0

[CLAUDE-3.5-SONNET]
repo_id = claude-3-5-sonnet-20240620
context_length = 200000
min_GPU_RAM = 0

[MISTRAL-7B-v0.1-INSTRUCT]
repo_id = mistralai/Mistral-7B-Instruct-v0.1
context_length = 8192
min_GPU_RAM = 30

[MISTRAL-7B-v0.1-INSTRUCT-AWQ]
repo_id = TheBloke/Mistral-7B-Instruct-v0.1-AWQ
tokenizer = mistralai/Mistral-7B-Instruct-v0.1
context_length = 8192
min_GPU_RAM = 8

[MISTRAL-7B-v0.2-INSTRUCT]
repo_id = mistralai/Mistral-7B-Instruct-v0.2
context_length = 32768
min_GPU_RAM = 30

[MISTRAL-7B-v0.2-INSTRUCT-AWQ]
repo_id = TheBloke/Mistral-7B-Instruct-v0.2-AWQ
tokenizer = mistralai/Mistral-7B-Instruct-v0.2
context_length = 32768
min_GPU_RAM = 8

[MISTRAL-8x7B-v0.1-INSTRUCT]
repo_id = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 32768
min_GPU_RAM = 120

[MISTRAL-8x7B-v0.1-INSTRUCT-PPLX]
repo_id = mixtral-8x7b-instruct
tokenizer = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 16384
min_GPU_RAM = 0

[MISTRAL-8x22B-PPLX]
repo_id = mixtral-8x22b-instruct
tokenizer = mistralai/Mixtral-8x22B-Instruct-v0.1
context_length = 16384
min_GPU_RAM = 0

[MISTRAL-8x7B-v0.1-INSTRUCT-GROQ]
repo_id = mixtral-8x7b-32768
tokenizer = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 32768
min_GPU_RAM = 0

[MISTRAL-8x7B-v0.1-INSTRUCT-AWQ]
repo_id = casperhansen/mixtral-instruct-awq
tokenizer = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 32768
min_GPU_RAM = 28

[ZEPHYR-7B-BETA]
repo_id = HuggingFaceH4/zephyr-7b-beta
context_length = 8096
min_GPU_RAM = 30

[ZEPHYR-7B-BETA-AWQ]
repo_id = TheBloke/zephyr-7B-beta-AWQ
tokenizer = HuggingFaceH4/zephyr-7b-beta
context_length = 8096
min_GPU_RAM = 8

[OPENCHAT-3.5]
repo_id = openchat/openchat_3.5
context_length = 8096
min_GPU_RAM = 30

[OPENCHAT-3.5-AWQ]
repo_id = TheBloke/openchat_3.5-AWQ
tokenizer = openchat/openchat_3.5
context_length = 8096
min_GPU_RAM = 8

[STARLING-7B-ALPHA]
repo_id = berkeley-nest/Starling-LM-7B-alpha
context_length = 8096
min_GPU_RAM = 30

[STARLING-7B-ALPHA-AWQ]
repo_id = TheBloke/Starling-LM-7B-alpha-AWQ
tokenizer = berkeley-nest/Starling-LM-7B-alpha
context_length = 8096
min_GPU_RAM = 8

[YI-6B-CHAT]
repo_id = 01-ai/Yi-6B-Chat
context_length = 4096
min_GPU_RAM = 20

[YI-6B-CHAT-AWQ]
repo_id = 01-ai/Yi-6B-Chat-4bits
tokenizer = 01-ai/Yi-6B-Chat
context_length = 4096
min_GPU_RAM = 5

[YI-34B-CHAT]
repo_id = 01-ai/Yi-34B-Chat
context_length = 4096
min_GPU_RAM = 80

[YI-34B-CHAT-AWQ]
repo_id = TheBloke/Yi-34B-Chat-AWQ
tokenizer = 01-ai/Yi-34B-Chat
context_length = 4096
min_GPU_RAM = 20

[SOLAR-10.7B-INSTRUCT-1.0]
repo_id = upstage/SOLAR-10.7B-Instruct-v1.0
context_length = 4096
min_GPU_RAM = 30

[SOLAR-10.7B-INSTRUCT-1.0-AWQ]
repo_id = TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ
tokenizer = upstage/SOLAR-10.7B-Instruct-v1.0
context_length = 4096
min_GPU_RAM = 7

[GEMMA-2-9B]
repo_id = google/gemma-2-9b-it
context_length = 8192

[GEMMA-2-9B-GROQ]
repo_id = gemma2-9b-it
tokenizer = google/gemma-2-9b-it
context_length = 8192

[GEMMA-2-9B-GGUF]
repo_id = bartowski/gemma-2-9b-it-GGUF
file_name = gemma-2-9b-it-Q4_K_M.gguf
tokenizer = google/gemma-2-9b-it
context_length = 8192

[GEMMA-2-27B]
repo_id = google/gemma-2-27b-it
context_length = 8192

[GEMMA-2-27B-TGTR]
repo_id = google/gemma-2-27b-it
tokenizer = google/gemma-2-27b-it
context_length = 8192

[GEMMA-2-27B-GGUF]
repo_id = bartowski/gemma-2-27b-it-GGUF
file_name = gemma-2-27b-it-Q3_K_M.gguf
tokenizer = google/gemma-2-27b-it
context_length = 8192

[COMMANDR-4BIT]
repo_id = CohereForAI/c4ai-command-r-v01-4bit
tokenizer = CohereForAI/c4ai-command-r-v01-4bit
context_length = 128000

[COMMANDR-PLUS-4BIT]
repo_id = CohereForAI/c4ai-command-r-plus-4bit
tokenizer = CohereForAI/c4ai-command-r-plus-4bit
context_length = 128000

[GEMINI-1.5-FLASH]
repo_id = gemini-1.5-flash-latest
context_length = 1000000

[GEMINI-1.5-PRO]
repo_id = gemini-1.5-pro
context_length = 1000000