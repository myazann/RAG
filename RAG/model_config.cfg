[New model]

[VICUNA-7B-v1.5-GPTQ]
repo_id = TheBloke/vicuna-7B-v1.5-GPTQ
context_length = 4096
min_GPU_RAM = 8

[VICUNA-13B-v1.5-GPTQ]
repo_id = TheBloke/vicuna-13B-v1.5-GPTQ
context_length = 4096
min_GPU_RAM = 12

[LLAMA2-7B-HF]
repo_id = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = 28

[LLAMA2-7B-GPTQ]
repo_id = TheBloke/Llama-2-7b-Chat-GPTQ
basename = model
context_length = 4096
min_GPU_RAM = 8

[LLAMA2-13B-GPTQ]
repo_id = TheBloke/Llama-2-13B-chat-GPTQ
basename = model
context_length = 4096
min_GPU_RAM = 12

[LLAMA2-70B-GGUF]
repo_id = TheBloke/Llama-2-70B-chat-GGUF
basename = llama-2-70b-chat.Q4_K_M.gguf
tokenizer = meta-llama/Llama-2-70b-chat-hf
context_length = 4096
min_GPU_RAM = 40

[STABLE-BELUGA-7B-GGUF]
repo_id = TheBloke/StableBeluga-7B-GGUF
basename = stablebeluga-7b.Q4_K_M.gguf
tokenizer = TheBloke/StableBeluga-7B-GPTQ
context_length = 4096
min_GPU_RAM = 8

[STABLE-BELUGA-7B-GPTQ]
repo_id = TheBloke/StableBeluga-7B-GPTQ
context_length = 4096
min_GPU_RAM = 8

[STABLE-BELUGA-13B-GPTQ]
repo_id = TheBloke/StableBeluga-13B-GPTQ
context_length = 4096
min_GPU_RAM = 12

[CLAUDE-V1]
repo_id = claude-1.2
context_length = 100000
min_GPU_RAM = 0

[CLAUDE-V2]
repo_id = claude-2.0
context_length = 100000
min_GPU_RAM = 0

[LUNA-UNCENSORED]
repo_id = TheBloke/Luna-AI-Llama2-Uncensored-GPTQ
basename = model
context_length = 4096
min_GPU_RAM = 8