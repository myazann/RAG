[VICUNA-7B-v1.5]
repo_id = lmsys/vicuna-7b-v1.5
context_length = 4096
min_GPU_RAM = 24

[VICUNA-7B-v1.5-AWQ]
repo_id = TheBloke/vicuna-7B-v1.5-AWQ
tokenizer = lmsys/vicuna-7b-v1.5
context_length = 4096
min_GPU_RAM = 8

[VICUNA-7B-v1.5-GPTQ]
repo_id = TheBloke/vicuna-7B-v1.5-GPTQ
tokenizer = lmsys/vicuna-7b-v1.5
context_length = 4096
min_GPU_RAM = 8

[VICUNA-7B-16K-v1.5]
repo_id = lmsys/vicuna-7b-v1.5-16k
context_length = 16384
min_GPU_RAM = 24

[VICUNA-7B-16K-v1.5-AWQ]
repo_id = TheBloke/vicuna-7B-v1.5-16K-AWQ
tokenizer = lmsys/vicuna-7b-v1.5-16k
context_length = 16384
min_GPU_RAM = 8

[VICUNA-7B-16K-v1.5-GPTQ]
repo_id = TheBloke/vicuna-7B-v1.5-16K-GPTQ
tokenizer = lmsys/vicuna-7b-v1.5-16k
context_length = 16384
min_GPU_RAM = 8

[VICUNA-13B-v1.5]
repo_id = lmsys/vicuna-13b-v1.5
context_length = 4096
min_GPU_RAM = 34

[VICUNA-13B-v1.5-AWQ]
repo_id = TheBloke/vicuna-13B-v1.5-AWQ
tokenizer = lmsys/vicuna-13b-v1.5
context_length = 4096
min_GPU_RAM = 12

[VICUNA-13B-v1.5-GPTQ]
repo_id = TheBloke/vicuna-13B-v1.5-GPTQ
tokenizer = lmsys/vicuna-13b-v1.5
context_length = 4096
min_GPU_RAM = 12

[VICUNA-13B-16K-v1.5]
repo_id = lmsys/vicuna-13b-v1.5-16k
context_length = 16384
min_GPU_RAM = 34

[VICUNA-13B-16K-v1.5-AWQ]
repo_id = TheBloke/vicuna-13B-v1.5-16K-AWQ
tokenizer = lmsys/vicuna-13b-v1.5-16k
context_length = 16384
min_GPU_RAM = 12

[VICUNA-13B-16K-v1.5-GPTQ]
repo_id = TheBloke/vicuna-13B-v1.5-16K-GPTQ
tokenizer = lmsys/vicuna-13b-v1.5-16k
context_length = 4096
min_GPU_RAM = 12

[LLAMA2-7B]
repo_id = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = 24

[LLAMA2-7B-AWQ]
repo_id = TheBloke/Llama-2-7B-Chat-AWQ
tokenizer = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = 8

[LLAMA2-7B-GPTQ]
repo_id = TheBloke/Llama-2-7B-Chat-GPTQ
tokenizer = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = 4

[LLAMA2-13B]
repo_id = meta-llama/Llama-2-13b-chat-hf
context_length = 4096
min_GPU_RAM = 34

[LLAMA2-13B-AWQ]
repo_id = TheBloke/Llama-2-13B-chat-AWQ
tokenizer = meta-llama/Llama-2-13b-chat-hf
context_length = 4096
min_GPU_RAM = 12

[LLAMA2-13B-GPTQ]
repo_id = TheBloke/Llama-2-13B-chat-GPTQ
tokenizer = meta-llama/Llama-2-13b-chat-hf
context_length = 4096
min_GPU_RAM = 8

[LLAMA2-70B]
repo_id = meta-llama/Llama-2-70b-chat-hf
context_length = 4096
min_GPU_RAM = 150

[LLAMA2-70B-AWQ]
repo_id = TheBloke/Llama-2-70B-Chat-AWQ
tokenizer = meta-llama/Llama-2-70b-chat-hf
context_length = 4096
min_GPU_RAM = 40

[LLAMA2-70B-GPTQ]
repo_id = TheBloke/Llama-2-70B-Chat-GPTQ
tokenizer = meta-llama/Llama-2-70b-chat-hf
context_length = 4096
min_GPU_RAM = 40

[CHATGPT-3.5]
repo_id = gpt-3.5-turbo
context_length = 16384
min_GPU_RAM = 0

[CHATGPT-4]
repo_id = gpt-4
context_length = 128000
min_GPU_RAM = 0

[CLAUDE-1.2]
repo_id = claude-instant-1.2
context_length = 100000
min_GPU_RAM = 0

[CLAUDE-2.0]
repo_id = claude-2.0
context_length = 100000
min_GPU_RAM = 0

[CLAUDE-2.1]
repo_id = claude-2.1
context_length = 200000
min_GPU_RAM = 0

[CLAUDE-3-OPUS]
repo_id = claude-3-opus-20240229
context_length = 200000
min_GPU_RAM = 0

[CLAUDE-3-SONNET]
repo_id = claude-3-sonnet-20240229
context_length = 200000
min_GPU_RAM = 0

[MISTRAL-7B-v0.2-INSTRUCT]
repo_id = mistralai/Mistral-7B-Instruct-v0.2
context_length = 8096
min_GPU_RAM = 30

[MISTRAL-7B-v0.2-INSTRUCT-AWQ]
repo_id = TheBloke/Mistral-7B-Instruct-v0.2-AWQ
tokenizer = mistralai/Mistral-7B-Instruct-v0.2
context_length = 8096
min_GPU_RAM = 8

[MISTRAL-7B-v0.2-INSTRUCT-GPTQ]
repo_id = TheBloke/Mistral-7B-Instruct-v0.2-GPTQ
tokenizer = mistralai/Mistral-7B-Instruct-v0.2
context_length = 8096
min_GPU_RAM = 8

[MISTRAL-8x7B-v0.1-INSTRUCT]
repo_id = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 32768
min_GPU_RAM = 120

[MISTRAL-8x7B-v0.1-INSTRUCT-PPLX]
repo_id = mixtral-8x7b-instruct
tokenizer = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 8192
min_GPU_RAM = 0

[MISTRAL-8x7B-v0.1-INSTRUCT-GROQ]
repo_id = mixtral-8x7b-32768
tokenizer = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 32768
min_GPU_RAM = 0

[MISTRAL-8x7B-v0.1-INSTRUCT-GPTQ]
repo_id = TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
tokenizer = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 32768
min_GPU_RAM = 28

[MISTRAL-8x7B-v0.1-INSTRUCT-AWQ]
repo_id = casperhansen/mixtral-instruct-awq
tokenizer = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 32768
min_GPU_RAM = 28

[ZEPHYR-7B-BETA]
repo_id = HuggingFaceH4/zephyr-7b-beta
context_length = 8096
min_GPU_RAM = 30

[ZEPHYR-7B-BETA-AWQ]
repo_id = TheBloke/zephyr-7B-beta-AWQ
tokenizer = HuggingFaceH4/zephyr-7b-beta
context_length = 8096
min_GPU_RAM = 8

[ZEPHYR-7B-BETA-GPTQ]
repo_id = TheBloke/zephyr-7B-beta-GPTQ
tokenizer = HuggingFaceH4/zephyr-7b-beta
context_length = 8096
min_GPU_RAM = 8

[OPENCHAT-3.5]
repo_id = openchat/openchat_3.5
context_length = 8096
min_GPU_RAM = 30

[OPENCHAT-3.5-AWQ]
repo_id = TheBloke/openchat_3.5-AWQ
tokenizer = openchat/openchat_3.5
context_length = 8096
min_GPU_RAM = 8

[OPENCHAT-3.5-GPTQ]
repo_id = TheBloke/openchat_3.5-GPTQ
tokenizer = openchat/openchat_3.5
context_length = 8096
min_GPU_RAM = 8

[STARLING-7B-ALPHA]
repo_id = berkeley-nest/Starling-LM-7B-alpha
context_length = 8096
min_GPU_RAM = 30

[STARLING-7B-ALPHA-AWQ]
repo_id = TheBloke/Starling-LM-7B-alpha-AWQ
tokenizer = berkeley-nest/Starling-LM-7B-alpha
context_length = 8096
min_GPU_RAM = 8

[STARLING-7B-ALPHA-GPTQ]
repo_id = TheBloke/Starling-LM-7B-alpha-GPTQ
tokenizer = berkeley-nest/Starling-LM-7B-alpha
context_length = 8096
min_GPU_RAM = 8

[YI-6B-CHAT]
repo_id = 01-ai/Yi-6B-Chat
context_length = 4096
min_GPU_RAM = 20

[YI-6B-CHAT-AWQ]
repo_id = 01-ai/Yi-6B-Chat-4bits
tokenizer = 01-ai/Yi-6B-Chat
context_length = 4096
min_GPU_RAM = 5

[YI-34B-CHAT]
repo_id = 01-ai/Yi-34B-Chat
context_length = 4096
min_GPU_RAM = 80

[YI-34B-CHAT-AWQ]
repo_id = TheBloke/Yi-34B-Chat-AWQ
tokenizer = 01-ai/Yi-34B-Chat
context_length = 4096
min_GPU_RAM = 20

[YI-34B-CHAT-GPTQ]
repo_id = TheBloke/Yi-34B-Chat-GPTQ
tokenizer = 01-ai/Yi-34B-Chat
context_length = 4096
min_GPU_RAM = 20

[SOLAR-10.7B-INSTRUCT-1.0]
repo_id = upstage/SOLAR-10.7B-Instruct-v1.0
context_length = 4096
min_GPU_RAM = 30

[SOLAR-10.7B-INSTRUCT-1.0-AWQ]
repo_id = TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ
tokenizer = upstage/SOLAR-10.7B-Instruct-v1.0
context_length = 4096
min_GPU_RAM = 7

[STABLELM-2-ZEPHYR-1.6B]
repo_id = stabilityai/stablelm-2-zephyr-1_6b
context_length = 4096

[PHI2]
repo_id = microsoft/phi-2
context_length = 2048

[GEMMA-2B-IT]
repo_id = google/gemma-2b-it
context_length = 8192

[GEMMA-7B-IT]
repo_id = google/gemma-7b-it
context_length = 8192