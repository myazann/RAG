[VICUNA-7B-v1.5]
repo_id = lmsys/vicuna-7b-v1.5
context_length = 4096
min_GPU_RAM = 28

[VICUNA-7B-v1.5-GGUF]
repo_id = TheBloke/vicuna-7B-v1.5-GGUF
tokenizer = TheBloke/Llama-2-7B-chat-GPTQ
context_length = 4096
min_GPU_RAM = 8

[VICUNA-7B-16K-v1.5-GGUF]
repo_id = TheBloke/vicuna-7B-v1.5-16K-GGUF
tokenizer = TheBloke/Llama-2-7B-chat-GPTQ
context_length = 16384
min_GPU_RAM = 8

[VICUNA-13B-v1.5-GGUF]
repo_id = TheBloke/vicuna-13B-v1.5-GGUF
tokenizer = TheBloke/Llama-2-7B-chat-GPTQ
context_length = 4096
min_GPU_RAM = 12

[VICUNA-13B-16K-v1.5-GGUF]
repo_id = TheBloke/vicuna-13B-v1.5-16K-GGUF
tokenizer = TheBloke/Llama-2-13B-chat-GPTQ
context_length = 16384
min_GPU_RAM = 12

[LLAMA2-7B]
repo_id = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = 38

[LLAMA2-7B-GPTQ]
repo_id = TheBloke/Llama-2-7b-chat-GPTQ
context_length = 4096
min_GPU_RAM = 4

[LLAMA2-7B-GGUF]
repo_id = TheBloke/Llama-2-7b-Chat-GGUF
tokenizer = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = 8

[LLAMA2-13B]
repo_id = meta-llama/Llama-2-13b-chat-hf
context_length = 4096
min_GPU_RAM = 80

[LLAMA2-13B-GPTQ]
repo_id = TheBloke/Llama-2-13B-Chat-GPTQ
context_length = 4096
min_GPU_RAM = 8

[LLAMA2-13B-GGUF]
repo_id = TheBloke/Llama-2-13B-chat-GGUF
tokenizer = meta-llama/Llama-2-13b-chat-hf
context_length = 4096
min_GPU_RAM = 12

[LLAMA2-70B]
repo_id = meta-llama/Llama-2-70b-chat-hf
context_length = 4096
min_GPU_RAM = 150

[LLAMA2-70B-GPTQ]
repo_id = TheBloke/Llama-2-70B-chat-GPTQ
context_length = 4096
min_GPU_RAM = 40

[LLAMA2-70B-GGUF]
repo_id = TheBloke/Llama-2-70B-chat-GGUF
tokenizer = meta-llama/Llama-2-70b-chat-hf
context_length = 4096
min_GPU_RAM = 46

[STABLE-BELUGA-7B]
repo_id = stabilityai/StableBeluga-7B
context_length = 4096
min_GPU_RAM = 38

[STABLE-BELUGA-7B-GGUF]
repo_id = TheBloke/StableBeluga-7B-GGUF
tokenizer = TheBloke/StableBeluga-7B-GPTQ
context_length = 4096
min_GPU_RAM = 8

[STABLE-BELUGA-13B-GGUF]
repo_id = TheBloke/StableBeluga-13B-GGUF
tokenizer = TheBloke/StableBeluga-13B-GPTQ
context_length = 4096
min_GPU_RAM = 12

[STABLE-BELUGA-70B-GGUF]
repo_id = TheBloke/StableBeluga2-70B-GGUF
tokenizer = TheBloke/StableBeluga-13B-GPTQ
context_length = 4096
min_GPU_RAM = 40

[CLAUDE-V1]
repo_id = claude-1.2
context_length = 100000
min_GPU_RAM = 0

[CLAUDE-V2]
repo_id = claude-2.0
context_length = 100000
min_GPU_RAM = 0

[LUNA-UNCENSORED]
repo_id = TheBloke/Luna-AI-Llama2-Uncensored-GGUF
tokenizer = TheBloke/StableBeluga-13B-GPTQ
context_length = 4096
min_GPU_RAM = 8