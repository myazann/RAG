[LLAMA-3.2-3B]
repo_id = meta-llama/Llama-3.2-3B-Instruct
context_length = 128000

[LLAMA-3.1-8B]
repo_id = meta-llama/Meta-Llama-3.1-8B-Instruct
context_length = 128000

[NEMOTRON-LLAMA-3.1-70B]
repo_id = nvidia/Llama-3.1-Nemotron-70B-Instruct-HF
context_length = 128000

[LLAMA-3.1-70B]
repo_id = meta-llama/Meta-Llama-3.1-70B-Instruct
context_length = 128000

[LLAMA-3.1-70B-GROQ]
repo_id = llama-3.1-70b-versatile
tokenizer = meta-llama/Meta-Llama-3.1-70B-Instruct
context_length = 8192

[LLAMA-3.1-70B-TGTR]
repo_id = meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
tokenizer = meta-llama/Meta-Llama-3.1-70B-Instruct
context_length = 128000

[LLAMA-3.1-70B-PPLX]
repo_id = llama-3.1-70b-instruct	
tokenizer = meta-llama/Meta-Llama-3.1-70B-Instruct
context_length = 131072

[LLAMA-3.1-405B-TGTR]
repo_id = meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo
tokenizer = meta-llama/Meta-Llama-3.1-405B-Instruct
context_length = 4096

[GPT-3.5]
repo_id = gpt-3.5-turbo
context_length = 16384
min_GPU_RAM = 0

[GPT-4]
repo_id = gpt-4-turbo
context_length = 128000
min_GPU_RAM = 0

[GPT-4o]
repo_id = gpt-4o-2024-08-06
context_length = 128000
min_GPU_RAM = 0

[GPT-4o-mini]
repo_id = gpt-4o-mini
context_length = 128000
min_GPU_RAM = 0

[CLAUDE-3-OPUS]
repo_id = claude-3-opus-20240229
context_length = 200000
min_GPU_RAM = 0

[CLAUDE-3-SONNET]
repo_id = claude-3-sonnet-20240229
context_length = 200000
min_GPU_RAM = 0

[CLAUDE-3-HAIKU]
repo_id = claude-3-haiku-20240307
context_length = 200000
min_GPU_RAM = 0

[CLAUDE-3.5-SONNET]
repo_id = claude-3-5-sonnet-20240620
context_length = 200000
min_GPU_RAM = 0

[MISTRAL-7B-v0.1-INSTRUCT]
repo_id = mistralai/Mistral-7B-Instruct-v0.1
context_length = 8192
min_GPU_RAM = 30

[MISTRAL-7B-v0.1-INSTRUCT-AWQ]
repo_id = TheBloke/Mistral-7B-Instruct-v0.1-AWQ
tokenizer = mistralai/Mistral-7B-Instruct-v0.1
context_length = 8192
min_GPU_RAM = 8

[MISTRAL-7B-v0.2-INSTRUCT]
repo_id = mistralai/Mistral-7B-Instruct-v0.2
context_length = 32768
min_GPU_RAM = 30

[MISTRAL-7B-v0.2-INSTRUCT-AWQ]
repo_id = TheBloke/Mistral-7B-Instruct-v0.2-AWQ
tokenizer = mistralai/Mistral-7B-Instruct-v0.2
context_length = 32768
min_GPU_RAM = 8

[MINISTRAL-8B-INSTRUCT]
repo_id = mistralai/Ministral-8B-Instruct-2410
context_length = 32768

[OPENCHAT-3.5]
repo_id = openchat/openchat_3.5
context_length = 8096
min_GPU_RAM = 30

[OPENCHAT-3.5-AWQ]
repo_id = TheBloke/openchat_3.5-AWQ
tokenizer = openchat/openchat_3.5
context_length = 8096
min_GPU_RAM = 8

[GEMMA-2-2B]
repo_id = google/gemma-2-2b-it
context_length = 8192

[GEMMA-2-9B]
repo_id = google/gemma-2-9b-it
context_length = 8192

[GEMMA-2-9B-GROQ]
repo_id = gemma2-9b-it
tokenizer = google/gemma-2-9b-it
context_length = 8192

[GEMMA-2-9B-GGUF]
repo_id = bartowski/gemma-2-9b-it-GGUF
file_name = gemma-2-9b-it-Q4_K_M.gguf
tokenizer = google/gemma-2-9b-it
context_length = 8192

[GEMMA-2-27B]
repo_id = google/gemma-2-27b-it
context_length = 8192

[GEMMA-2-27B-TGTR]
repo_id = google/gemma-2-27b-it
tokenizer = google/gemma-2-27b-it
context_length = 8192

[GEMMA-2-27B-GGUF]
repo_id = bartowski/gemma-2-27b-it-GGUF
file_name = gemma-2-27b-it-Q3_K_M.gguf
tokenizer = google/gemma-2-27b-it
context_length = 8192

[GEMINI-1.5-FLASH]
repo_id = gemini-1.5-flash-latest
context_length = 1000000

[GEMINI-1.5-PRO]
repo_id = gemini-1.5-pro
context_length = 1000000

[PHI-3.5-MINI-INSTRUCT]
repo_id = microsoft/Phi-3.5-mini-instruct
context_length = 128000

[QWEN-2-7B-INSTRUCT]
repo_id = Qwen/Qwen2-7B-Instruct
context_length = 131072