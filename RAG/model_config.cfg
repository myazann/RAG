[VICUNA-7B-v1.5]
repo_id = lmsys/vicuna-7b-v1.5
context_length = 4096
min_GPU_RAM = 24

[VICUNA-7B-v1.5-AWQ]
repo_id = TheBloke/vicuna-7B-v1.5-AWQ
context_length = 4096
min_GPU_RAM = 8

[VICUNA-7B-v1.5-GPTQ]
repo_id = TheBloke/vicuna-7B-v1.5-GPTQ
context_length = 4096
min_GPU_RAM = 8

[VICUNA-7B-v1.5-GGUF]
repo_id = TheBloke/vicuna-7B-v1.5-GGUF
tokenizer = TheBloke/Llama-2-7B-chat-GPTQ
context_length = 4096
min_GPU_RAM = 8

[VICUNA-7B-16K-v1.5]
repo_id = lmsys/vicuna-7b-v1.5-16k
context_length = 16384
min_GPU_RAM = 24

[VICUNA-7B-16K-v1.5-AWQ]
repo_id = TheBloke/vicuna-7B-v1.5-16K-AWQ
context_length = 16384
min_GPU_RAM = 8

[VICUNA-7B-16K-v1.5-GPTQ]
repo_id = TheBloke/vicuna-7B-v1.5-16K-GPTQ
context_length = 16384
min_GPU_RAM = 8

[VICUNA-7B-16K-v1.5-GGUF]
repo_id = TheBloke/vicuna-7B-v1.5-16K-GGUF
tokenizer = TheBloke/Llama-2-7B-chat-GPTQ
context_length = 16384
rope_freq_scale = 0.25
min_GPU_RAM = 8

[VICUNA-13B-v1.5]
repo_id = lmsys/vicuna-13b-v1.5
context_length = 4096
min_GPU_RAM = 34

[VICUNA-13B-v1.5-AWQ]
repo_id = TheBloke/vicuna-13B-v1.5-AWQ
context_length = 4096
min_GPU_RAM = 12

[VICUNA-13B-v1.5-GPTQ]
repo_id = TheBloke/vicuna-13B-v1.5-GPTQ
context_length = 4096
min_GPU_RAM = 12

[VICUNA-13B-v1.5-GGUF]
repo_id = TheBloke/vicuna-13B-v1.5-GGUF
tokenizer = TheBloke/Llama-2-7B-chat-GPTQ
context_length = 4096
min_GPU_RAM = 12

[VICUNA-13B-16K-v1.5]
repo_id = lmsys/vicuna-13b-v1.5-16k
context_length = 16384
min_GPU_RAM = 34

[VICUNA-13B-16K-v1.5-AWQ]
repo_id = TheBloke/vicuna-13B-v1.5-16K-AWQ
context_length = 16384
min_GPU_RAM = 12

[VICUNA-13B-16K-v1.5-GPTQ]
repo_id = TheBloke/vicuna-13B-v1.5-16K-GPTQ
context_length = 4096
min_GPU_RAM = 12

[VICUNA-13B-16K-v1.5-GGUF]
repo_id = TheBloke/vicuna-13B-v1.5-16K-GGUF
tokenizer = TheBloke/Llama-2-13B-chat-GPTQ
context_length = 16384
rope_freq_scale = 0.25
min_GPU_RAM = 12

[LLAMA2-7B]
repo_id = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = 24

[LLAMA2-7B-AWQ]
repo_id = TheBloke/Llama-2-7B-Chat-AWQ
context_length = 4096
min_GPU_RAM = 8

[LLAMA2-7B-GPTQ]
repo_id = TheBloke/Llama-2-7B-Chat-GPTQ
context_length = 4096
min_GPU_RAM = 4

[LLAMA2-7B-GGUF]
repo_id = TheBloke/Llama-2-7B-Chat-GGUF
tokenizer = meta-llama/Llama-2-7b-chat-hf
context_length = 4096
min_GPU_RAM = 8

[LLAMA2-13B]
repo_id = meta-llama/Llama-2-13b-chat-hf
context_length = 4096
min_GPU_RAM = 34

[LLAMA2-13B-AWQ]
repo_id = TheBloke/Llama-2-13B-chat-AWQ
context_length = 4096
min_GPU_RAM = 12

[LLAMA2-13B-GPTQ]
repo_id = TheBloke/Llama-2-13B-chat-GPTQ
context_length = 4096
min_GPU_RAM = 8

[LLAMA2-13B-GGUF]
repo_id = TheBloke/Llama-2-13B-Chat-GGUF
tokenizer = meta-llama/Llama-2-13b-chat-hf
context_length = 4096
min_GPU_RAM = 12

[LLAMA2-70B]
repo_id = meta-llama/Llama-2-70b-chat-hf
context_length = 4096
min_GPU_RAM = 150

[LLAMA2-70B-AWQ]
repo_id = TheBloke/Llama-2-70B-Chat-AWQ
context_length = 4096
min_GPU_RAM = 40

[LLAMA2-70B-GPTQ]
repo_id = TheBloke/Llama-2-70B-Chat-GPTQ
context_length = 4096
min_GPU_RAM = 40

[LLAMA2-70B-GGUF]
repo_id = TheBloke/Llama-2-70B-Chat-GGUF
tokenizer = meta-llama/Llama-2-70b-chat-hf
context_length = 4096
min_GPU_RAM = 46

[CHATGPT-3.5]
repo_id = gpt-3.5-turbo
context_length = 4096
min_GPU_RAM = 0

[CHATGPT-3.5-16K]
repo_id = gpt-3.5-turbo-16k
context_length = 16384
min_GPU_RAM = 0

[CHATGPT-4]
repo_id = gpt-4
context_length = 4096
min_GPU_RAM = 0

[CHATGPT-4-32K]
repo_id = gpt-4-32k
context_length = 32768
min_GPU_RAM = 0

[CLAUDE-V1]
repo_id = claude-1.2
context_length = 100000
min_GPU_RAM = 0

[CLAUDE-V2]
repo_id = claude-2.0
context_length = 100000
min_GPU_RAM = 0

[MISTRAL-7B-v0.1-INSTRUCT]
repo_id = mistralai/Mistral-7B-Instruct-v0.1
context_length = 8096
min_GPU_RAM = 30

[MISTRAL-7B-v0.1-INSTRUCT-AWQ]
repo_id = TheBloke/Mistral-7B-Instruct-v0.1-AWQ
context_length = 8096
min_GPU_RAM = 8

[MISTRAL-7B-v0.1-INSTRUCT-GPTQ]
repo_id = TheBloke/Mistral-7B-Instruct-v0.1-GPTQ
context_length = 8096
min_GPU_RAM = 8

[MISTRAL-7B-v0.1-INSTRUCT-GGUF]
repo_id = TheBloke/Mistral-7B-Instruct-v0.1-GGUF
tokenizer = mistralai/Mistral-7B-Instruct-v0.1
context_length = 8096
min_GPU_RAM = 8

[MISTRAL-8x7B-v0.1-INSTRUCT]
repo_id = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 32768
min_GPU_RAM = 120

[MISTRAL-8x7B-v0.1-INSTRUCT-GGUF]
repo_id = TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF
tokenizer = mistralai/Mixtral-8x7B-Instruct-v0.1
context_length = 32768
min_GPU_RAM = 28

[MISTRAL-8x7B-v0.1-INSTRUCT-GPTQ]
repo_id = TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
context_length = 32768
min_GPU_RAM = 28

[MISTRAL-8x7B-v0.1-INSTRUCT-AWQ]
repo_id = TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ
context_length = 32768
min_GPU_RAM = 28

[ZEPHYR-7B-ALPHA]
repo_id = HuggingFaceH4/zephyr-7b-alpha
context_length = 8096
min_GPU_RAM = 30

[ZEPHYR-7B-ALPHA-AWQ]
repo_id = TheBloke/zephyr-7B-alpha-AWQ
context_length = 8096
min_GPU_RAM = 8

[ZEPHYR-7B-ALPHA-GPTQ]
repo_id = TheBloke/zephyr-7B-alpha-GPTQ
context_length = 8096
min_GPU_RAM = 8

[ZEPHYR-7B-ALPHA-GGUF]
repo_id = TheBloke/zephyr-7B-alpha-GGUF
tokenizer = HuggingFaceH4/zephyr-7b-alpha
context_length = 8096
min_GPU_RAM = 8

[ZEPHYR-7B-BETA]
repo_id = HuggingFaceH4/zephyr-7b-beta
context_length = 8096
min_GPU_RAM = 30

[ZEPHYR-7B-BETA-AWQ]
repo_id = TheBloke/zephyr-7B-beta-AWQ
context_length = 8096
min_GPU_RAM = 8

[ZEPHYR-7B-BETA-GPTQ]
repo_id = TheBloke/zephyr-7B-beta-GPTQ
context_length = 8096
min_GPU_RAM = 8

[ZEPHYR-7B-BETA-GGUF]
repo_id = TheBloke/zephyr-7B-beta-GGUF
tokenizer = HuggingFaceH4/zephyr-7b-beta
context_length = 8096
min_GPU_RAM = 8

[OPENCHAT-3.5]
repo_id = openchat/openchat_3.5
context_length = 8096
min_GPU_RAM = 30

[OPENCHAT-3.5-AWQ]
repo_id = TheBloke/openchat_3.5-AWQ
context_length = 8096
min_GPU_RAM = 8

[OPENCHAT-3.5-GPTQ]
repo_id = TheBloke/openchat_3.5-GPTQ
context_length = 8096
min_GPU_RAM = 8

[OPENCHAT-3.5-GGUF]
repo_id = TheBloke/openchat_3.5-GGUF
tokenizer = openchat/openchat_3.5
context_length = 8096
min_GPU_RAM = 8

[STARLING-7B-ALPHA]
repo_id = berkeley-nest/Starling-LM-7B-alpha
context_length = 8096
min_GPU_RAM = 30

[STARLING-7B-ALPHA-AWQ]
repo_id = TheBloke/Starling-LM-7B-alpha-AWQ
context_length = 8096
min_GPU_RAM = 8

[STARLING-7B-ALPHA-GPTQ]
repo_id = TheBloke/Starling-LM-7B-alpha-GPTQ
context_length = 8096
min_GPU_RAM = 8

[STARLING-7B-ALPHA-GGUF]
repo_id = TheBloke/Starling-LM-7B-alpha-GGUF
tokenizer = berkeley-nest/Starling-LM-7B-alpha
context_length = 8096
min_GPU_RAM = 8

[YI-6B-CHAT]
repo_id = 01-ai/Yi-6B-Chat
context_length = 4096
min_GPU_RAM = 20

[YI-6B-CHAT-AWQ]
repo_id = 01-ai/Yi-6B-Chat-4bits
context_length = 4096
min_GPU_RAM = 5

[YI-34B-CHAT]
repo_id = 01-ai/Yi-34B-Chat
context_length = 4096
min_GPU_RAM = 80

[YI-34B-CHAT-AWQ]
repo_id = TheBloke/Yi-34B-Chat-AWQ
context_length = 4096
min_GPU_RAM = 20

[YI-34B-CHAT-GPTQ]
repo_id = TheBloke/Yi-34B-Chat-GPTQ
context_length = 4096
min_GPU_RAM = 20

[YI-34B-CHAT-GGUF]
repo_id = TheBloke/Yi-34B-Chat-GGUF
tokenizer = 01-ai/Yi-34B-Chat
context_length = 4096
min_GPU_RAM = 20

[SOLAR-10.7B-INSTRUCT-1.0]
repo_id = upstage/SOLAR-10.7B-Instruct-v1.0
context_length = 4096
min_GPU_RAM = 30

[SOLAR-10.7B-INSTRUCT-1.0-AWQ]
repo_id = TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ
tokenizer = upstage/SOLAR-10.7B-Instruct-v1.0
context_length = 4096
min_GPU_RAM = 7

[STABLELM-2-ZEPHYR-1.6B]
repo_id = stabilityai/stablelm-2-zephyr-1_6b
context_length = 4096